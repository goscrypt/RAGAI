<!DOCTYPE html><html><head><meta charset="UTF-8"><style>        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f9;
        }

        #app-container {
            display: flex;
            width: 100%;
            height: 100vh;
            max-width: 1400px;
            margin: 0 auto;
            background-color: #fff;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }

        /* --- LEFT PANEL: Chat Interface and File Upload --- */
        #left-panel {
            width: 60%;
            padding: 20px;
            display: flex;
            flex-direction: column;
            border-right: 1px solid #ccc;
        }

        #left-panel h2 {
            color: #2c3e50;
            margin-top: 0;
            padding-bottom: 10px;
            border-bottom: 2px solid #3498db;
        }
        
        /* File Upload and Indexing Section (KB Control) */
        #file-control-section {
            padding-bottom: 15px;
            margin-bottom: 15px;
            border-bottom: 1px solid #eee;
        }

        #file-input {
            margin-right: 10px;
        }

        #model-status {
            margin-top: 10px;
            font-size: 0.9em;
            color: #2980b9;
        }
        
        /* Chat History / Conversation Area */
        #chat-history {
            flex-grow: 1;
            overflow-y: auto;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 4px;
            margin-bottom: 15px;
            background-color: #fefefe;
        }

        /* Query Input Section */
        #input-section {
            display: flex;
        }

        #query-input {
            flex-grow: 1;
            padding: 10px;
            border: 1px solid #ccc;
            border-radius: 4px 0 0 4px;
            font-size: 16px;
        }

        #send-button {
            padding: 10px 15px;
            background-color: #3498db;
            color: white;
            border: none;
            border-radius: 0 4px 4px 0;
            cursor: pointer;
            font-size: 16px;
        }

        #send-button:hover {
            background-color: #2980b9;
        }

        /* --- RIGHT PANEL: Debug and Configuration --- */
        
        #right-panel {
            display: flex;	
            flex-direction: column;
            width: 40%;	
            max-height: 100vh;	
            overflow-y: hidden;	
            gap: 10px;	
            padding: 20px;
        }

        #right-panel h2 {
            color: #2c3e50;
            margin-top: 0;
            padding-bottom: 10px;
            border-bottom: 2px solid #e67e22;
            flex-shrink: 0;	
        }

        #config-settings {
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 4px;
            background-color: #f9f9f9;
            flex-shrink: 0;	
            display: grid;	
            grid-template-columns: repeat(2, 1fr);
            gap: 10px;
            margin-bottom: 10px;	
        }
        
        .config-full-width {
            grid-column: 1 / -1;
        }
        
        /* New styling for fallback checkbox group */
        #fallback-settings {
            grid-column: 1 / -1;
            padding-top: 5px;
            border-top: 1px dashed #ddd;
            display: flex;
            flex-wrap: wrap;
            gap: 8px 15px;
            font-size: 0.9em;
        }
        #fallback-settings div {
            flex-shrink: 0;
        }
        
        /* New styling for control groups */
        .control-group {
            display: flex;
            justify-content: space-between;
            align-items: center;
            gap: 10px;
        }
        
        /* Fieldset styling for grouping sampling/beam controls */
        #decoding-controls fieldset {
            border: 1px solid #ccc;
            padding: 10px;
            margin-top: 10px;
            border-radius: 4px;
        }
        #decoding-controls legend {
            padding: 0 10px;
            font-weight: bold;
            color: #2c3e50;
        }

        #prompt-input {
            width: 100%;
            min-height: 80px;
            box-sizing: border-box;
            resize: vertical;
        }
        
        /* Debug Area Container now holds only one panel */
        #debug-area-container {
            display: flex;
            flex-direction: column;
            flex-grow: 1;
            gap: 10px;
            min-height: 0;	
        }

        /* Base style for all major debug sections */
        .debug-panel {
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 4px;
            background-color: #f9f9f9;
            
            flex-grow: 1;	
            min-height: 0;	
            display: flex;	
            flex-direction: column;	
            overflow: hidden;	
        }
        
        .debug-panel h3 {
            margin-top: 0;
            font-size: 1.1em;
            border-bottom: 1px solid #eee;
            padding-bottom: 5px;
            margin-bottom: 5px;
            color: #333;
            flex-shrink: 0;
            display: flex;	
            justify-content: space-between;
            align-items: center;
        }
        
        /* The Debug View (Context Output) now takes up all the available space */
        #debug-view {
            flex-grow: 1;
        }
        
        /* Context output area */
        #context-output {
            flex-grow: 1;
            white-space: pre-wrap;
            overflow-y: auto;
            padding: 10px;
            background-color: #f0f0f0;
            border: 1px dashed #ccc;
            font-family: monospace;
            font-size: 0.9em;
            border-radius: 3px;
        }
        
        /* Chat styling remains the same */
        .chat-message {
            margin-bottom: 10px;
            padding: 8px 12px;
            border-radius: 18px;
            max-width: 80%;
            clear: both;
            font-size: 0.95em;
        }

        .user-message {
            float: right;
            background-color: #3498db;
            color: white;
            border-bottom-right-radius: 2px;
        }

        .bot-message {
            float: left;
            background-color: #ecf0f1;
            color: #2c3e50;
            border-bottom-left-radius: 2px;
        }
        
        .data-message {
            clear: both;
            width: 98%;
            margin: 10px 0;
            padding: 10px;
            background-color: #f0f8ff;
            border: 1px solid #cceeff;
            font-family: monospace;
            font-size: 0.8em;
            white-space: pre-wrap;
        }
        
        .grounded-label {
            font-weight: bold;
            margin-right: 5px;
            color: #27ae60;
        }
        /* Style for range inputs */
        input[type="range"] {
            width: 60%; 
            min-width: 100px; 
        }
</style></head><body><!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RAG Chatbot with Dynamic Controls</title>
</head>
<body>

    <div id="app-container">
        
        <div id="left-panel">
            <h2>RAG Chat Interface</h2>

            <div id="file-control-section">
                <input type="file" id="file-input" accept=".txt,.json,.csv">
                <button id="index-button">Index File</button>
                <div id="model-status">Model: Loading... | KB: Empty</div>
            </div>

            <div id="chat-history">
                <div class="chat-message bot-message">Welcome!</div>
            </div>

            <div id="input-section">
                <input type="text" id="query-input" placeholder="Ask a question..." disabled>
                <button id="send-button" disabled>Send</button>
            </div>
        </div>

        <div id="right-panel">
            <h2>Configuration & Debug</h2>

            <div id="config-settings">
             <select id="generatorModelSelect">
              <option value="squad">SQuAD QA</option>
              <option value="flan-t5">Flan-T5 Generator</option>
              <option value="gemini">Gemini Generator</option> 
             </select>
                        
                <div class="config-full-width">
                    <div>RAG Prompt: <input type="text" id="topic-input" value="Vinia" style="width: 50px;margin-bottom:10px"></div>
                    <textarea id="prompt-input" rows="4">Answer in complete sentences using the provided context.
</textarea>
                </div>
                
                <div id="decoding-controls" class="config-full-width">
                    <div class="control-group">
                        <label for="useSamplingCheckbox"><strong>Use Sampling (Top-p / Temp)</strong></label>
                        <input type="checkbox" id="useSamplingCheckbox" checked>
                    </div>

                    <fieldset id="samplingControls">
                        <legend>Sampling Parameters</legend>
                        
                        <div class="control-group">
                            <label for="temperatureInput">Temperature:</label>
                            <input type="range" id="temperatureInput" min="0.0" max="1.0" step="0.05" value="0.3">
                            <output id="tempOutput" style="width: 30px;">0.3</output>
                        </div>

                        <div class="control-group">
                            <label for="topPInput">Top P:</label>
                            <input type="range" id="topPInput" min="0.0" max="1.0" step="0.05" value="0.4">
                            <output id="topPOutput" style="width: 30px;">0.4</output>
                        </div>
                         
                    </fieldset>

                    <fieldset id="beamControls" style="display: none;">
                        <legend>Beam Search Parameters</legend>

                        <div class="control-group">
                            <label for="numBeamsInput">Number of Beams:</label>
                            <input type="range" id="numBeamsInput" min="1" max="10" step="1" value="1">
                            <output id="numBeamsOutput" style="width: 30px;">1</output>
                        </div>
                          
                    </fieldset>
                      <label for="num-samples-input">Num Samples:</label>
						<input type="number" id="num-samples-input" value="3" min="1" max="10">
                          <label for="no-repeat-ngram-size">No Repeat</label>
                        <input type="number" id="no-repeat-ngram-size" value="3" min="1" max="3">
                       
                </div>
                <div id="fallback-settings">
                    <strong>Fallback Sources:</strong>
                    <div>
                        <input type="checkbox" id="fallback-ddg" >
                        <label for="fallback-ddg">DuckDuckGo</label>
                    </div>
                    <div>
                        <input type="checkbox" id="fallback-wiki" >
                        <label for="fallback-wiki">Wikipedia</label>
                    </div>
                    <div>
                        <input type="checkbox" id="fallback-op" checked>
                        <label for="fallback-op">One Piece</label>
                    </div>
                    <div>
                        <input type="checkbox" id="fallback-google" >
                        <label for="fallback-google">Google Sheet</label>
                    </div>
                </div>

                <div>
                    <div>Chunk Size (chars): <input type="number" id="chunk-size-input" value="256" min="100" max="2048" style="width: 60px;"></div>
                </div>
                <div>
                    <div>Chunk Overlap: <input type="number" id="chunk-overlap-input" value="32" min="0" max="500" style="width: 60px;"></div>
                </div>

                <div>
                    <div>Initial Retrieval (K): <input type="number" id="retrieve-k-input" value="5" min="1" max="20" style="width: 60px;"></div>
                </div>
                <div>
                    <div>Similarity Threshold: <input type="number" id="similarity-threshold-input" value="0.3" step="0.05" min="0.0" max="1.0" style="width: 60px;"></div>
                </div>
                <div>
                    <div>Final Candidates (N): <input type="number" id="rerank-n-input" value="3" min="1" max="10" style="width: 60px;"></div>
                </div>
                
                <div>
                    <div>Max New Tokens: <input type="number" id="max-new-tokens-input" value="200" min="10" max="512" style="width: 60px;"></div>
                </div>
                <div>
                    <div>Repetition Penalty: <input type="number" id="repetition-penalty-input" value="1.05" step="0.1" min="0.5" max="2.0" style="width: 60px;"></div>
                </div>
                <div>
                    <div>Length Penalty: <input type="number" id="length-penalty-input" value="0.8" step="0.1" min="0.5" max="2.0" style="width: 60px;"></div>
                </div>
                
                <input type="hidden" id="reindex-button" value="placeholder">
                <input type="hidden" id="show-chunking-input" checked>
                <input type="hidden" id="user-input">
                <input type="hidden" id="chat-window">
                <input type="hidden" id="system-status">
                <input type="hidden" id="chunk-list">
                <input type="hidden" id="retrieved-chunks-container">
            </div>
            
        <div id="debug-area-container">
            <div id="debug-view" class="debug-panel">
                <h3>
                    Debug View: Retrieved Contexts
                    <button id="save-fallback-kb-btn" class="utility-button" style="float: right; margin-left: 10px;">Save Debug Content</button>
                </h3>
                <pre id="context-output">Awaiting first search...</pre>
                <div id="rerank-details" style="font-size: 0.85em; margin-top: 5px;">
                    Top Score: N/A | Candidates: 0
                </div>
            </div>
        </div>

        </div>
    </div>
    
    
<div id="system-status"></div>

              
<script type="module">
import { pipeline, AutoTokenizer, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.0';

// --- MODEL IDS ---
const EMBEDDER_MODEL = 'Xenova/all-MiniLM-L6-v2';
const SQUAD_MODELÂ  Â  = 'Xenova/distilbert-base-uncased-distilled-squad';
const FLAN_T5_MODELÂ  = 'Xenova/LaMini-Flan-T5-77M';

// --- STATE ---
let vectorStore = null;
let knowledgeBaseText = null;
let embedder = null;
let squadPipeline = null;
let flanT5Pipeline = null;
let currentFileExtension = null;

// --- DOM ELEMENTS (Assumed to exist in your HTML) ---
const chatWindow = document.getElementById('chat-history');
const systemStatus = document.getElementById('model-status'); 
const fileInput = document.getElementById('file-input');
const reindexButton = document.getElementById('index-button');
const generatorModelSelect = document.getElementById('generatorModelSelect');
const userInput = document.getElementById('query-input');
const sendButton = document.getElementById('send-button');
const promptInput = document.getElementById('prompt-input');
const topicInput = document.getElementById('topic-input');

const chunkList = document.getElementById('context-output');
const rerankDetails = document.getElementById('rerank-details');

const chunkSizeInput = document.getElementById('chunk-size-input');
const chunkOverlapInput = document.getElementById('chunk-overlap-input');
const retrieveKInput = document.getElementById('retrieve-k-input');
const similarityThresholdInput = document.getElementById('similarity-threshold-input');
const rerankNInput = document.getElementById('rerank-n-input');

const maxNewTokensInput = document.getElementById('max-new-tokens-input');
const repetitionPenaltyInput = document.getElementById('repetition-penalty-input');
const lengthPenaltyInput = document.getElementById('length-penalty-input');

// --- DECODING STRATEGY DOM ELEMENTS ---
const useSamplingCheckbox = document.getElementById('useSamplingCheckbox');
const temperatureInput = document.getElementById('temperatureInput');
const topPInput = document.getElementById('topPInput');
const numBeamsInput = document.getElementById('numBeamsInput');
const noRrepeatNgramInput = document.getElementById('no-repeat-ngram-size');
// --- END DECODING ELEMENTS ---

const fallbackDdg = document.getElementById('fallback-ddg');
const fallbackWiki = document.getElementById('fallback-wiki');
const fallbackOP = document.getElementById('fallback-op');
const fallbackGoogle = document.getElementById('fallback-google');

async function initializeModels() {
Â  try {
Â  Â  if (systemStatus) systemStatus.textContent = 'Model: Loading... | KB: Empty';

Â  Â  // Load all three pipelines in parallel
Â  Â  const [embedderPipeline, squad, flan] = await Promise.all([
Â  Â  Â  pipeline('feature-extraction', EMBEDDER_MODEL),
Â  Â  Â  pipeline('question-answering', SQUAD_MODEL),
Â  Â  Â  pipeline('text2text-generation', FLAN_T5_MODEL)
Â  Â  ]);

Â  Â  embedder = embedderPipeline;
Â  Â  squadPipeline = squad;
Â  Â  flanT5Pipeline = flan;

Â  Â  if (systemStatus) systemStatus.textContent = 'Model: Ready | KB: 0 chunks';
Â  Â  console.log('SYSTEM: All models initialized:', {
Â  Â  Â  embedder: EMBEDDER_MODEL,
Â  Â  Â  squad: SQUAD_MODEL,
Â  Â  Â  flan: FLAN_T5_MODEL
Â  Â  });
Â  } catch (err) {
Â  Â  console.error('Model Loading Error:', err);
Â  Â  if (systemStatus) systemStatus.textContent = `SYSTEM ERROR: ${err.message}`;
Â  }
}

initializeModels();

fileInput.addEventListener('change', () => {
Â  reindexButton.disabled = fileInput.files.length === 0;
Â  if (fileInput.files.length) currentFileExtension = fileInput.files[0].name.split('.').pop().toLowerCase();
});

reindexButton.addEventListener('click', async () => {
Â  const file = fileInput.files[0];
Â  if (!file) return alert('Select a file first.');

Â  chatWindow.innerHTML = ''; // Clear chat before starting the process

Â  const reader = new FileReader();
Â  reader.onload = async (e) => {
Â  Â  knowledgeBaseText = e.target.result;

Â  Â  // DYNAMICALLY READS CHUNKING SETTINGS from UI
Â  Â  vectorStore = new VectorStore(embedder,
Â  Â  Â  parseInt(chunkSizeInput.value),
Â  Â  Â  parseInt(chunkOverlapInput.value)
Â  Â  );
Â  Â  await vectorStore.indexText(knowledgeBaseText); // Indexing happens here

Â  Â  // --- LOGIC: DISPLAY CHUNKS IN CHAT BOX ---
Â  Â  const numChunks = vectorStore.chunks.length;

Â  Â  // 1. Add a message summarizing the indexing
Â  Â  appendMessage('bot', `Knowledge base "${file.name}" indexed successfully! There are **${numChunks}** chunks. See below for the raw chunk texts.`);

Â  Â  // 2. Format and display raw chunk data in a special container
Â  Â  const chunkTextOutput = vectorStore.chunks.map((c, index) =>
Â  Â  Â  `[CHUNK ${index + 1}/${numChunks}]\n${c.text}`
Â  Â  ).join('\n\n---\n\n');

Â  Â  const dataDiv = document.createElement('div');
Â  Â  dataDiv.className = 'data-message';
Â  Â  dataDiv.textContent = chunkTextOutput;
Â  Â  chatWindow.appendChild(dataDiv);
Â  Â  chatWindow.scrollTop = chatWindow.scrollHeight;
Â  Â  // --- END LOGIC ---

Â  Â  userInput.disabled = false;
Â  Â  sendButton.disabled = false;
Â  Â  userInput.focus();
Â  };
Â  reader.readAsText(file);
});

// --- VECTOR STORE CLASS (Final, Corrected Version) ---
class VectorStore {
Â  constructor(embedder, chunkSize, chunkOverlap) {
Â  Â  this.embedder = embedder;
Â  Â  this.chunkSize = chunkSize;
Â  Â  this.chunkOverlap = chunkOverlap;
Â  Â  this.chunks = [];
Â  Â  this.tokenizer = null;
Â  Â  this.hashSet = new Set();
Â  }

Â  async initialize() {
Â  Â  try {
Â  Â  Â  this.tokenizer = await AutoTokenizer.from_pretrained(EMBEDDER_MODEL);
Â  Â  Â  console.log("[VECTOR STORE] Tokenizer loaded.");
Â  Â  } catch (err) {
Â  Â  Â  console.error("[VECTOR STORE] Tokenizer failed to load:", err);
Â  Â  }
Â  }

Â  /**
Â  Â * CHUNKTEXT: MODIFIED TO RESPECT BLOCK BOUNDARIES
Â  Â * The array blockStartIndices contains the sentence indices that MUST NOT 
Â  Â * be included in the overlap of the preceding chunk (i.e., start of a new block).
Â  Â */
Â  chunkText(sentences, blockStartIndices) {
Â  Â  const chunks = [];
Â  Â  let sentenceIndex = 0;

Â  Â  while (sentenceIndex < sentences.length) {
Â  Â  Â  let currentChunkSentences = [];
Â  Â  Â  let currentLength = 0;
Â  Â  Â  let lastSentenceIndexInChunk = sentenceIndex;
Â  Â  Â  
Â  Â  Â  // Find the next hard block boundary index *after* the current sentenceIndex
Â  Â  Â  const nextBlockBoundary = blockStartIndices.find(index => index > sentenceIndex) || sentences.length;

Â  Â  Â  // 1. Accumulate sentences for the current chunk
Â  Â  Â  // Loop runs only up to the next boundary OR the max chunk size
Â  Â  Â  for (let i = sentenceIndex; i < nextBlockBoundary; i++) { 
Â  Â  Â  Â  const sentence = sentences[i];
Â  Â  Â  Â  const sentenceLength = sentence.length + 1; // +1 for space/punctuation
Â  Â  Â  Â  
Â  Â  Â  Â  if (currentLength + sentenceLength > this.chunkSize && currentChunkSentences.length > 0) {
Â  Â  Â  Â  Â  break;
Â  Â  Â  Â  }
Â  Â  Â  Â  currentChunkSentences.push(sentence);
Â  Â  Â  Â  currentLength += sentenceLength;
Â  Â  Â  Â  lastSentenceIndexInChunk = i;
Â  Â  Â  }
Â  Â  Â  
Â  Â  Â  if (currentChunkSentences.length > 0) {
Â  Â  Â  Â  chunks.push(currentChunkSentences.join(' ').trim());
Â  Â  Â  }
Â  Â  Â  
Â  Â  Â  if (lastSentenceIndexInChunk === sentences.length - 1) {
Â  Â  Â  Â  break;
Â  Â  Â  }

Â  Â  Â  // 2. Calculate Overlap Start Index (Standard Logic)
Â  Â  Â  let overlapLength = 0;
Â  Â  Â  let sentencesToKeepForOverlap = 0;
Â  Â  Â  
Â  Â  Â  for (let i = currentChunkSentences.length - 1; i >= 0; i--) {
Â  Â  Â  Â  if (overlapLength + currentChunkSentences[i].length > this.chunkOverlap && sentencesToKeepForOverlap > 0) {
Â  Â  Â  Â  Â  break;
Â  Â  Â  Â  }
Â  Â  Â  Â  overlapLength += currentChunkSentences[i].length + 1;
Â  Â  Â  Â  sentencesToKeepForOverlap++;
Â  Â  Â  }
Â  Â  Â  
Â  Â  Â  const intendedNextChunkStartIndex = lastSentenceIndexInChunk - sentencesToKeepForOverlap + 1;
Â  Â  Â  
Â  Â  Â  // 3. CRITICAL FIX: Overlap Termination at Block Boundary
Â  Â  Â  // The next chunk MUST NOT start before the next block boundary index.
Â  Â  Â  const actualNextChunkStartIndex = nextBlockBoundary;

Â  Â  Â  // Set the index for the next iteration: take the intended overlap start, 
Â  Â  Â  // but ensure it's at least the block boundary. This removes all overlap at the stopper.
Â  Â  Â  sentenceIndex = Math.max(intendedNextChunkStartIndex, actualNextChunkStartIndex);
Â  Â  }
Â  Â  return chunks;
Â  }

Â  hashChunk(text) {
Â  Â  // Only use lower case for hashing
Â  Â  const clean = text.replace(/\s+/g, " ").trim().toLowerCase();
Â  Â  let hash = 0;
Â  Â  for (let i = 0; i < clean.length; i++) {
Â  Â  Â  hash = (hash << 5) - hash + clean.charCodeAt(i);
Â  Â  Â  hash |= 0;
Â  Â  }
Â  Â  return hash;
Â  }

Â  isNearDuplicate(prev, current) {
Â  Â  if (!prev || !current) return false;
Â  Â  // Use lower case for comparison to catch near-duplicates regardless of case
Â  Â  const prevLower = prev.toLowerCase();
Â  Â  const currentLower = current.toLowerCase();
Â  Â  const shorter = prevLower.length < currentLower.length ? prevLower : currentLower;
Â  Â  const longer = prevLower.length < currentLower.length ? currentLower : prevLower;
Â  Â  return longer.includes(shorter) && (shorter.length / longer.length > 0.8);
Â  }

Â  /**
Â  Â * INDEXTEXT: Handles block indices from the splitter.
Â  Â */
Â  async indexText(text) {
Â  Â  this.chunks = [];
Â  Â  this.hashSet.clear();
Â  Â  if (!this.tokenizer) await this.initialize();

Â  Â  const cleanText = text.replace(/[\u200B-\u200F\uFEFF]/g, " ").trim();
Â  Â  const { sentences: allSentences, blockStartIndices } = splitIntoSentences(cleanText); 
Â  Â  
Â  Â  const rawChunks = this.chunkText(allSentences, blockStartIndices);

Â  Â  console.log(`[VECTOR STORE] Indexing ${rawChunks.length} raw chunks from ${allSentences.length} sentences...`);

Â  Â  let lastChunk = ""; 
Â  Â  let lastEmbedding = null;

Â  Â  for (const chunkText of rawChunks) {
Â  Â  Â  const hash = this.hashChunk(chunkText);

Â  Â  Â  if (this.hashSet.has(hash)) {
Â  Â  Â  Â  console.log("[DEDUP] Skipping exact duplicate:", chunkText.slice(0, 60) + "...");
Â  Â  Â  Â  continue;
Â  Â  Â  }

Â  Â  Â  if (this.isNearDuplicate(lastChunk, chunkText)) {
Â  Â  Â  Â  console.log("[DEDUP] Skipping overlap duplicate:", chunkText.slice(0, 60) + "...");
Â  Â  Â  Â  continue;
Â  Â  Â  }

Â  Â  Â  try {
Â  Â  Â  Â  const output = await this.embedder(chunkText, { pooling: "mean", normalize: true });
Â  Â  Â  Â  const embedding = Array.from(output.data);

Â  Â  Â  Â  if (lastEmbedding) {
Â  Â  Â  Â  Â  const sim = this.cosineSimilarity(embedding, lastEmbedding);
Â  Â  Â  Â  Â  if (sim > 0.97) {
Â  Â  Â  Â  Â  Â  console.log("[DEDUP] Skipping semantic duplicate:", chunkText.slice(0, 60) + "...");
Â  Â  Â  Â  Â  Â  continue;
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }

Â  Â  Â  Â  this.hashSet.add(hash);
Â  Â  Â  Â  // Store the original-cased text
Â  Â  Â  Â  this.chunks.push({ text: chunkText, embedding });
Â  Â  Â  Â  lastChunk = chunkText;
Â  Â  Â  Â  lastEmbedding = this.chunks[this.chunks.length - 1].embedding;

Â  Â  Â  } catch (err) {
Â  Â  Â  Â  console.error("[VECTOR STORE] Embedding failed:", err);
Â  Â  Â  }
Â  Â  }

Â  Â  if (systemStatus) systemStatus.textContent = `Model: Ready | KB: ${this.chunks.length} unique chunks indexed.`;
Â  }

Â  cosineSimilarity(a, b) {
Â  Â  let dot = 0, magA = 0, magB = 0;
Â  Â  for (let i = 0; i < a.length; i++) {
Â  Â  Â  dot += a[i] * b[i];
Â  Â  Â  magA += a[i] ** 2;
Â  Â  Â  magB += b[i] ** 2;
Â  Â  }
Â  Â  return magA && magB ? dot / (Math.sqrt(magA) * Math.sqrt(magB)) : 0;
Â  }

Â  async search(query, k, rerankN, threshold) {
Â  Â  if (!this.chunks.length) return [];
Â  Â  const output = await this.embedder(query, { pooling: "mean", normalize: true });
Â  Â  const queryEmbedding = Array.from(output.data);
Â  Â  
Â  Â  const scores = this.chunks.map(c => ({
Â  Â  Â  text: c.text, // Use the original-cased text
Â  Â  Â  score: this.cosineSimilarity(queryEmbedding, c.embedding)
Â  Â  }));
Â  Â  
Â  Â  const results = scores.filter(s => s.score >= threshold).sort((a, b) => b.score - a.score).slice(0, k);
Â  Â  const reranked = results.slice(0, rerankN);

Â  Â  // **FIX: Ensure correct case and line breaks for the UI display**
Â  Â  chunkList.textContent = results.map(r => {
Â  Â  Â  Â  let chunkText = (typeof r.text === 'string') ? r.text : (typeof r.text === 'object' ? JSON.stringify(r.text) : String(r.text ?? ""));
Â  Â  Â  Â  const cleanedDisplayText = chunkText.replace(/[\s\*]+/g, ' ').trim(); 
Â  Â  Â  Â  return `Score: ${r.score.toFixed(4)}\n${cleanedDisplayText}`;
Â  Â  }).join("\n\n---\n\n");
Â  Â  rerankDetails.textContent = `Top Score: ${reranked[0]?.score.toFixed(4) ?? "N/A"} | Candidates: ${reranked.length}`;

Â  Â  return reranked;
Â  }
}
// --- END VECTOR STORE CLASS ---

// ----------------------------------------------------------------------
// --- SENTENCE SPLITTING FUNCTIONS (Modified to return block indices) ---
// ----------------------------------------------------------------------

/**
 * SPLITINTOSENTENCES: MODIFIED TO RETURN SENTENCES AND BLOCK START INDICES
 */
function splitIntoSentences(text) {
Â  const lines = text.split('\n');
Â  let allSentences = [];
Â  let blockStartIndices = [0]; // Index 0 is always a block start
Â  
Â  let currentPartLines = [];

Â  for (const line of lines) {
Â  Â  const stopperIndex = line.indexOf('//');
Â  Â  let content = line.trim();

Â  Â  if (stopperIndex !== -1) {
Â  Â  Â  // 1. Local Discard: Keep content BEFORE the stopper
Â  Â  Â  content = line.substring(0, stopperIndex).trim();

Â  Â  Â  // If there was content before the stopper, process it
Â  Â  Â  if (content.length > 0) {
Â  Â  Â  Â  currentPartLines.push(content);
Â  Â  Â  }
Â  Â  Â  
Â  Â  Â  // Process the accumulated content as a single block
Â  Â  Â  if (currentPartLines.length > 0) {
Â  Â  Â  Â  const blockText = currentPartLines.join(' ');
Â  Â  Â  Â  const newSentences = processTextBlock(blockText);
Â  Â  Â  Â  allSentences.push(...newSentences);
Â  Â  Â  }
Â  Â  Â  
Â  Â  Â  // 2. Hard Block Separation: Mark the start of the next block
Â  Â  Â  currentPartLines = [];
Â  Â  Â  blockStartIndices.push(allSentences.length);
Â  Â  Â  
Â  Â  Â  continue;
Â  Â  }

Â  Â  // If no stopper, and the line has content, add it to the current block
Â  Â  if (content.length > 0) {
Â  Â  Â  currentPartLines.push(content);
Â  Â  }
Â  }

Â  // Process the last accumulated block
Â  if (currentPartLines.length > 0) {
Â  Â  const blockText = currentPartLines.join(' ');
Â  Â  const newSentences = processTextBlock(blockText);
Â  Â  allSentences.push(...newSentences);
Â  }
Â  
Â  // Ensure the last block start index is valid and unique
Â  if (blockStartIndices[blockStartIndices.length - 1] === allSentences.length) {
Â  Â  blockStartIndices.pop();
Â  }

Â  // Fallback logic
Â  if (allSentences.length === 0 && text.trim().length > 0) {
Â  Â  const sentences = text.split(/\n{3,}/).filter(s => s.trim().length > 0).map(s => s.trim());
Â  Â  return { sentences: sentences, blockStartIndices: [0] };
Â  }

Â  return { sentences: allSentences, blockStartIndices };
}

/**
 * Helper function to apply the sentence split logic to a clean block of text.
 */
function processTextBlock(part) {
Â  let sentences = [];
Â  
Â  // Split the text into parts based on strong paragraph breaks (\n\n) first
Â  const strongParts = part.split(/\n{2,}/);
Â  
Â  for (const strongPart of strongParts) {
Â  Â  if (strongPart.trim().length === 0) continue;

Â  Â  // Sentence splitting regex
Â  Â  const sentenceRegex = /(?<!\b(?:Dr|Mr|Ms|Mrs|Jr|Sr|vs|d|e\.g|i\.e|D))([.?!])\s+(?=[A-Z0-9*#-]|$)/g;
Â  Â  const preprocessed = strongPart.replace(/---/g, ' ');
Â  Â  const rawParts = preprocessed.split(sentenceRegex);

Â  Â  for (let i = 0; i < rawParts.length; i += 2) {
Â  Â  Â  let sentence = rawParts[i];
Â  Â  Â  // Re-append the punctuation
Â  Â  Â  if (i + 1 < rawParts.length && rawParts[i + 1] && rawParts[i + 1].match(/[.?!]/)) {
Â  Â  Â  Â  sentence += rawParts[i + 1];
Â  Â  Â  }
Â  Â  Â  if (sentence.trim().length > 0) {
Â  Â  Â  Â  sentences.push(sentence.trim());
Â  Â  Â  }
Â  Â  }
Â  }

Â  return sentences;
}

// ----------------------------------------------------------------------
// --- RAG AND UI FUNCTIONS (Rest of the script) ---
// ----------------------------------------------------------------------

function appendMessage(sender, msg, grounded = false) {
Â  const div = document.createElement('div');
Â  div.className = `chat-message ${sender}-message`;
Â  div.innerHTML = grounded ? `<span class="grounded-label">Grounded:</span> ${msg}` : msg;
Â  chatWindow.appendChild(div);
Â  chatWindow.scrollTop = chatWindow.scrollHeight;
}

sendButton.addEventListener('click', async () => {
Â  const query = userInput.value.trim();
Â  if (!query) return;
Â  userInput.value = '';
Â  runRAG(query);
});

userInput.addEventListener('keypress', (e) => {
Â  if (e.key === 'Enter') sendButton.click();
});


/**
Â * Trims text to ensure the final answer is less than or equal to maxTokens,
Â * while respecting sentence boundaries (no partial sentences).
Â */
function trimToTokenLimit(text, maxTokens) {
Â  const sentences = text.match(/[^.!?]+[.!?]+/g) || [text];
Â  let finalAnswerSentences = [];
Â  let currentWordCount = 0;

Â  for (const sentence of sentences) {
Â  Â  const sentenceWords = sentence.trim().split(/\s+/).filter(word => word.length > 0);
Â  Â  const sentenceWordCount = sentenceWords.length;
Â  Â  if (currentWordCount + sentenceWordCount <= maxTokens) {
Â  Â  Â  finalAnswerSentences.push(sentence);
Â  Â  Â  currentWordCount += sentenceWordCount;
Â  Â  } else {
Â  Â  Â  break;
Â  Â  }
Â  }
Â  return finalAnswerSentences.join(' ').trim();
}


const { GoogleGenAI } = require("@google/genai"); 
const ai = new GoogleGenAI(process.env.GEMINI_API_KEY || "AIzaSyA0JLeaXdO138mSnQG433a9YiBrnnemsTs");                                                         
// Global scope or accessible to runRAG
async function geminiGenerator(prompt, options) {
    // 1. Get the prompt (which includes the RAG instruction, context, and query)
    const fullPrompt = prompt;

    // 2. Configure the call using your API library (e.g., Google GenAI SDK)
    // NOTE: Replace this with your actual API key and setup logic
    try {
        // Example structure (will vary based on your SDK/API setup):
        const response = await fetch('/api/gemini/generate', {
             method: 'POST',
             headers: { 'Content-Type': 'application/json' },
             body: JSON.stringify({ 
                 prompt: fullPrompt, 
                 maxTokens: options.max_new_tokens 
             })
        });

        const data = await response.json();

        // 3. Return the generated text in a format that your Step 6 cleanup understands
        // For unified cleanup, we assume it returns a simple object: { generated_text: "..." }
        if (data.text) {
             return { generated_text: data.text };
        } else {
             throw new Error("Gemini API call failed or returned no text.");
        }

    } catch (error) {
        console.error("Gemini Generation Error:", error);
        throw error; 
    }
}
                                                         
async function runRAG(query) {
    // keep original signature (do not rename)
    try {
        // 1. Query Cleanup and Status Update
        if (typeof query !== 'string') {
            console.warn("runRAG called with non-string query:", query);
            query = String(query ?? "");
        }
        query = query.trim();
        if (!query.endsWith('?') && query.length > 0) query += '?';

        appendMessage('user', query);
        const systemStatus = document.getElementById('model-status');
        if (systemStatus) systemStatus.textContent = 'SYSTEM: Searching Knowledge Base...';

        // 2. Initializations & Model Selection
        const originalInstruction = (promptInput.value || "").trim().replace(/\s+$/, '');
        let finalAnswer = "";
        let context = "";
        let sourceTag = "";
        let topChunkText = ""; 
        let topChunkScore = 0.0;

        const selectedModelValue = generatorModelSelect.value || "";
        const isQuestionAnsweringModel = selectedModelValue.includes('squad');
        // --- NEW: Identify Gemini Model ---
        const isGeminiModel = selectedModelValue.includes('gemini'); 
        
        // Generator is set only if it's NOT SQuAD
        const generator = isQuestionAnsweringModel ? squadPipeline : (isGeminiModel ? geminiGenerator : flanT5Pipeline);
        // -----------------------------------

        const rerankN = parseInt(rerankNInput.value);
        const retrieveK = parseInt(retrieveKInput.value);
        const similarityThreshold = parseFloat(similarityThresholdInput.value);
        const topic = topicInput.value;

        // 3. Retrieval (Internal KB)
        let retrieved = await vectorStore.search(query, retrieveK, rerankN, similarityThreshold);
        
        // --- 50% TOP-SCORE FILTER (Existing Logic) ---
        if (retrieved && retrieved.length > 0) {
            
            const topChunk = retrieved[0];
            // Capture and clean the top chunk text immediately
            topChunkText = String(topChunk.text ?? "").replace(/[\s\*]+/g, ' ').trim(); 
            topChunkScore = topChunk.score;
            
            // ... (Logging logic for retrieved chunks omitted for brevity)
            
            if (retrieved.length > 1) { 
                const topScore = topChunk.score;
                const scoreThreshold = topScore * 0.50; 
                
                const originalCount = retrieved.length;
                retrieved = retrieved.filter(chunk => chunk.score >= scoreThreshold);
                
                if (originalCount !== retrieved.length) {
                    console.log(`[RAG Filter] Applied 50% top-score rule. Filtered ${originalCount - retrieved.length} chunks. New count: ${retrieved.length}`);
                }
            }
        }
        // --- END 50% TOP-SCORE FILTER ---


        if (retrieved && retrieved.length > 0) {
            // Context concatenation logic
            context = retrieved
                .map(r => String(r.text ?? "").replace(/[\s\*]+/g, ' ').trim())
                .filter(Boolean)
                .join("\n\n")
                .trim();

            sourceTag = "ðŸ“š";
        } else {
            sourceTag = "ðŸ§ ";
            // ... (Status updates for no context omitted for brevity)
            context = ""; 
        }

        // 4. Configure Generation Options and Construct Prompt
        let options = {};
        let prompt; 
        let qa_question; 
        const isUncasedModel = selectedModelValue.includes('uncased'); 

        if (isQuestionAnsweringModel) {
            if (systemStatus) systemStatus.textContent = 'SYSTEM: Generating response using QA (SQUAD) Model...';
            
            const cleanedQuery = query.replace(/\s+/g, ' ').trim();
            qa_question = isUncasedModel ? cleanedQuery.toLowerCase() : cleanedQuery;
            
            console.log(`DEBUG QA Call:Question: ${qa_question}, contextLen: ${context.length}`);

        } else {
            // --- T5 / GEMINI Generator Path ---
            if (systemStatus) systemStatus.textContent = `SYSTEM: Generating response using ${isGeminiModel ? 'Gemini' : 'Flan-T5'} Model...`;
            
            // The RAG prompt is unified for T5 and Gemini
            const dynamicInstruction = `${originalInstruction} Context: ${context} Question: ${query}`.trim().replace(/\s+/g, ' ');
            prompt = dynamicInstruction;
            
            // T5/Gemini options (assuming Gemini can use a subset or similar structure)
            const useSampling = useSamplingCheckbox.checked; 
            options = {
                max_new_tokens: parseInt(maxNewTokensInput.value),
                repetition_penalty: parseFloat(repetitionPenaltyInput.value),
                // ... other T5/Gemini options
                do_sample: useSampling,
            };
            // ... (Sampling/Beam search setup omitted for brevity)
        }

        // 5. Execute Generation / QA (safe calls)
        let results;
        try {
            if (isQuestionAnsweringModel) {
                if (!generator) throw new Error("QA model not initialized");
                results = await generator(qa_question, context);
            } 
            // --- NEW: Separate Gemini Call ---
            else if (isGeminiModel) {
                if (!generator) throw new Error("Gemini model not initialized");
                // Calling the placeholder function with the necessary RAG components
                results = await generator(prompt, options); 
            }
            // --- Existing T5 Call ---
            else {
                if (!generator) throw new Error("Generation model (T5) not initialized");
                results = await generator(prompt, options);
            }
        } catch (err) {
            console.error("Model call error:", err);
            appendMessage('bot', `(Error running model: ${err.message})`);
            if (systemStatus) systemStatus.textContent = `SYSTEM ERROR: ${err.message}`;
            return;
        }

        // 6. Cleanup and Final Selection Logic
        let cleanedText = "";

        if (isQuestionAnsweringModel) {
            // ... (Existing SQuAD output cleanup logic remains the same)
            let answer = null; // ... (extract answer logic)
            // ... (SQuAD logic uses "answer" and "confidence")
            // ... 
            
            if (typeof answer === 'string' && answer.trim().length > 0) {
                cleanedText = answer.trim();
                // (No truncation)
                
                console.log(`[QA RESULT] âœ… Answer: ${cleanedText} | score: ${confidence ?? "N/A"}`);
                
                if (cleanedText.length > 0) {
                    cleanedText = cleanedText.charAt(0).toUpperCase() + cleanedText.slice(1);
                }
            } else {
                console.warn("[QA RESULT] Unexpected QA output:", results);
                cleanedText = ""; 
            }
            
        } else {
            // --- T5 / GEMINI Output Cleanup Path ---
            // T5 and Gemini results are assumed to have a similar text output structure
            const rawGenerated = results?.generated_text || results?.[0]?.generated_text || String(results) || "";
            cleanedText = cleanGeneratedText(String(rawGenerated || ""), prompt, query);

            // optional trim
            const maxAnswerTokens = parseInt(maxNewTokensInput.value);
            if (maxAnswerTokens > 0 && cleanedText) {
                cleanedText = trimToTokenLimit(cleanedText, maxAnswerTokens);
            }
        }

        // 6b. Fallback Logic (T5 only if SQuAD failed)
        // Note: Gemini is NOT part of this fallback. If SQuAD fails, T5 is the fallback, 
        // regardless of whether Gemini was the original intent.

        const MIN_WORD_THRESHOLD = 7;Â 
        
        const noAnswer = !cleanedText || cleanedText.length === 0;
        const wordCount = cleanedText.split(/\s+/).filter(Boolean).length;
        const isTooShortAnswer = cleanedText.length > 0 && wordCount < MIN_WORD_THRESHOLD;

        if (isQuestionAnsweringModel && (noAnswer || isTooShortAnswer)) {
            // ... (Your existing SQuAD to T5 fallback logic remains here)
            try {
                let reason = noAnswer ? "No answer found" : `Answer too short (words: ${wordCount})`;
                console.warn(`[RAG Fallback] SQuAD result was: "${cleanedText}". Reason: ${reason}. Switching to T5 generator.`);
                
                const finalFallbackPrompt = `${originalInstruction} Context: ${context} Question: ${query}`.trim().replace(/\s+/g, ' ');

                const fallbackResults = await flanT5Pipeline(finalFallbackPrompt, {
                    max_new_tokens: Math.max(64, parseInt(maxNewTokensInput.value) || 128)
                });

                let fallbackText = ""; // ... (normalize fallback output)
                if (Array.isArray(fallbackResults) && fallbackResults[0]?.generated_text) {
                    fallbackText = fallbackResults[0].generated_text;
                } else if (typeof fallbackResults === 'object' && fallbackResults.generated_text) {
                    fallbackText = fallbackResults.generated_text;
                } else if (typeof fallbackResults === 'string') {
                    fallbackText = fallbackResults;
                }
                
                let tempCleanedText = cleanGeneratedText(String(fallbackText || ""), finalFallbackPrompt, query);

                let fallbackWordCount = tempCleanedText.split(/\s+/).filter(Boolean).length;
                
                if (fallbackWordCount >= MIN_WORD_THRESHOLD || tempCleanedText.toLowerCase().trim() === "i do not know") {
                    cleanedText = tempCleanedText;
                } else {
                    console.warn(`[RAG Fallback Fail] T5 answer still too short (words: ${fallbackWordCount}). Enforcing "i do not know".`);
                    cleanedText = "i do not know";
                }

            } catch (fbErr) {
                console.error("Fallback generation error:", fbErr);
                cleanedText = "I encountered an error during the fallback process and cannot provide an answer."; 
            }
        }

        // 7. Final Formatting and Display
        
        let finalOutput = cleanedText;
        
        // --- FINAL FAIL-SAFE: Use topChunkText if all else failed ---
        if ((!cleanedText || cleanedText.toLowerCase().trim() === "i do not know" || cleanedText.includes("error")) && topChunkText.length > 0 && topChunkScore > 0.6) {
             // Only overwrite if cleanedText is empty, 'i do not know', or an error, AND we have a high-confidence top chunk.
            console.warn("[RAG Final Output] Overriding model failure with top chunk text.");
            finalOutput = topChunkText.trim();
        } else if (!finalOutput || finalOutput.toLowerCase().trim() === "i do not know") {
            // Final, final fail-safe for an empty string or generic "i do not know"
            finalOutput = "I could not find a relevant answer in the knowledge base.";
        }
        
        finalAnswer = `${sourceTag}\n\n${finalOutput}`;
        // --- END FINAL FAIL-SAFE ---

        appendMessage('bot', finalAnswer);
        if (systemStatus) systemStatus.textContent = `Model: Ready | KB: ${vectorStore?.chunks?.length ?? 0} chunks.`;

    } catch (err) {
        // ... (Error handling)
        console.error("runRAG fatal error:", err);
        const systemStatus = document.getElementById('model-status');
        if (systemStatus) systemStatus.textContent = `SYSTEM ERROR: ${err.message}`;
        appendMessage('bot', `(Error: ${err.message})`);
    }
}
// --- END runRAG ---
</script>
</body>
</html><script>console.log('Code is running...');
/*notes
Add a force stopper in kb file to divide the chunks.
ex. // - this comment element is a stopper 


*/

document.addEventListener('DOMContentLoaded', () => {
            // --- 1. Link Range Sliders to Output Values (UI ONLY) ---
            function linkRangeSlider(inputId, outputId) {
                const input = document.getElementById(inputId);
                const output = document.getElementById(outputId);
                
                if (input && output) {
                    output.textContent = input.value; 
                    input.addEventListener('input', () => {
                        output.textContent = input.value;
                    });
                }
            }
            
            linkRangeSlider('temperatureInput', 'tempOutput');
            linkRangeSlider('topPInput', 'topPOutput');
            linkRangeSlider('numBeamsInput', 'numBeamsOutput');


            // --- 2. Toggle Sampling vs. Beam Search Panels (UI ONLY) ---
            const samplingCheckbox = document.getElementById('useSamplingCheckbox');
            const samplingPanel = document.getElementById('samplingControls');
            const beamPanel = document.getElementById('beamControls');
            
            function toggleDecodingMode() {
                if (samplingCheckbox.checked) {
                    samplingPanel.style.display = 'block';
                    beamPanel.style.display = 'none';
                } else {
                    beamPanel.style.display = 'block';
                    samplingPanel.style.display = 'none';
                }
            }

            toggleDecodingMode(); 
            samplingCheckbox.addEventListener('change', toggleDecodingMode);
        });
     

//cleaners and helpers

function cleanGeneratedText(generatedText, promptText, queryText) {
Â  let cleaned = (generatedText || "").trim();
Â  const fullInstruction = (promptText || "").trim();

Â  const escapedInstruction = fullInstruction.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
Â  const instructionRegex = new RegExp(`^${escapedInstruction}`, 'i');
Â  cleaned = cleaned.replace(instructionRegex, '').trim();

Â  const leakPhrases = [
Â  Â  /In summary,[\s\S]*/i,
Â  Â  /Strictly do not invent new words that are not found in context.[\s\S]*/i,
  	/not answered in the provided context.*/i,
Â  Â  /In this context,.*?\./i,
Â  Â  /context:.*question:.*answer:[\s\r\n]*/i,
Â  Â  /based on the context provided,*/i,
Â  Â  /based on the context,*/i,
Â  Â  /here is the answer based on the context:*/i,
Â  Â  /^that*/i,
Â  Â  /of a USE*/i,
Â  Â  /^yes,*/i,
  	/^the answer in this context is:*/i,
  	/not provided in the given context./i,
Â  Â  /of using a USE device*/i,
Â  Â  /answer:[\s\r\n]*/i,
Â  Â  /^The correct answer is:*/i,
Â  Â  /^The answer is:*/i,
  	/^The question is/i,
Â  Â  /Therefore, the answer is:*/i,
Â  Â  /Therefore, I cannot answer that question.*/i,
 	/^The given context does not provide information[\s\S]*/i,
  	/^The provided context[\s\S]*/i,
    /^The context does not provide information[\s\S]*/i,
  	/^It is not specified[\s\S]*/i,
Â  Â  /\[CONTEXT\]:*/i,
Â  Â  /\[QUESTION\]:*/i,
Â  ];

Â  try {
Â  Â  if (queryText) {
Â  Â  Â  leakPhrases.push(new RegExp(`^${queryText.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')}[\\s\\r\\n]*`, 'i'));
Â  Â  Â  leakPhrases.push(new RegExp(`^What is ${queryText.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')}[\\s\\r\n]*`, 'i'));
Â  Â  }
Â  } catch (e) { }

Â  for (const pattern of leakPhrases) {
Â  Â  cleaned = cleaned.replace(pattern, '').trim();
Â  }

Â  cleaned = cleaned.replace(/^['":-]/, '').trim();

Â  // Replace any sequence of asterisks and/or whitespace with a single space
Â  cleaned = cleaned.replace(/[\s\*]+/g, ' ').trim();

Â  return cleaned;
}<\/script></body></html>
